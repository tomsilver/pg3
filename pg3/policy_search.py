"""PG3 policy search."""

from typing import Callable, Dict, Iterator, List, Optional, Sequence, Set, \
    Tuple
from typing import Type as TypingType

from typing_extensions import TypeAlias

from pg3 import utils
from pg3.heuristics import _DemoPlanComparisonPG3Heuristic, _PG3Heuristic, \
    _PlanComparisonPG3Heuristic, _PolicyEvaluationPG3Heuristic, \
    _PolicyGuidedPG3Heuristic
from pg3.operators import _AddConditionPG3SearchOperator, \
    _AddRulePG3SearchOperator, _BottomUpPG3SearchOperator, \
    _DeleteConditionPG3SearchOperator, _DeleteRulePG3SearchOperator, \
    _PG3SearchOperator
from pg3.search import run_gbfs, run_hill_climbing
from pg3.structs import LiftedDecisionList, Predicate, STRIPSOperator, Task, \
    Trajectory, Type


def learn_policy(domain_str: str,
                 problem_strs: List[str],
                 horizon: int,
                 demos: Optional[List[List[str]]] = None,
                 max_rule_params: int = 8,
                 heuristic_name: str = "policy_guided",
                 search_method: str = "hill_climbing",
                 task_planning_heuristic: str = "lmcut",
                 max_policy_guided_rollout: int = 50,
                 gbfs_max_expansions: int = 100,
                 hc_enforced_depth: int = 0,
                 allow_new_vars: bool = True,
                 initial_policy_strs: Optional[List[str]] = None) -> str:
    """Outputs a string representation of a lifted decision list."""
    if demos is not None:
        assert len(demos) == len(problem_strs), "Supply one demo per problem."
        assert heuristic_name == "demo_plan_comparison", \
            ("Only supply demos if using demo_plan_comparison heuristic, and "
             "even then, the demos are optional.")
    types, predicates, operators = utils.parse_pddl_domain(domain_str)
    train_tasks = [
        utils.pddl_problem_str_to_task(problem_str, domain_str, types,
                                       predicates)
        for problem_str in problem_strs
    ]
    ldl = _run_policy_search(types, predicates, operators, train_tasks,
                             horizon, demos, max_rule_params, heuristic_name,
                             search_method, task_planning_heuristic,
                             max_policy_guided_rollout, gbfs_max_expansions,
                             hc_enforced_depth, allow_new_vars,
                             initial_policy_strs)
    return str(ldl)


def _run_policy_search(
        types: Set[Type],
        predicates: Set[Predicate],
        operators: Set[STRIPSOperator],
        train_tasks: Sequence[Task],
        horizon: int,
        demos: Optional[List[List[str]]] = None,
        max_rule_params: int = 8,
        heuristic_name: str = "policy_guided",
        search_method: str = "hill_climbing",
        task_planning_heuristic: str = "lmcut",
        max_policy_guided_rollout: int = 50,
        gbfs_max_expansions: int = 100,
        hc_enforced_depth: int = 0,
        allow_new_vars: bool = True,
        initial_policy_strs: Optional[List[str]] = None) -> LiftedDecisionList:
    """Search for a lifted decision list policy that solves the training
    tasks."""
    # Set up a search over LDL space.
    _S: TypeAlias = LiftedDecisionList
    # An "action" here is a search operator and an integer representing the
    # count of successors generated by that operator.
    _A: TypeAlias = Tuple[_PG3SearchOperator, int]

    # The heuristic is what distinguishes PG3 from baseline approaches.
    heuristic = _create_heuristic(heuristic_name, predicates, operators,
                                  train_tasks, horizon, demos,
                                  task_planning_heuristic,
                                  max_policy_guided_rollout)

    # TODO: refactor to produce a separate "plan generator" that is used by
    # both the heuristic and the search operators.
    assert isinstance(heuristic, _PlanComparisonPG3Heuristic)

    def _generate_plan_examples(
            ldl: LiftedDecisionList) -> Iterator[Trajectory]:
        for task_idx in range(len(train_tasks)):
            yield heuristic._get_plan_for_task(ldl, task_idx)

    # Create the PG3 search operators.
    search_operators = _create_search_operators(predicates, operators,
                                                _generate_plan_examples,
                                                allow_new_vars)
    get_successors = _operators_to_successor_fn(search_operators,
                                                max_rule_params)

    # Initialize the search with an empty list.
    if initial_policy_strs is None:
        initial_states = [LiftedDecisionList([])]
    else:
        initial_states = [
            utils.parse_ldl_from_str(l, types, predicates, operators)
            for l in initial_policy_strs
        ]

    if search_method == "gbfs":
        # Terminate only after max expansions.
        path, _ = run_gbfs(initial_states=initial_states,
                           check_goal=lambda _: False,
                           get_successors=get_successors,
                           heuristic=heuristic,
                           max_expansions=gbfs_max_expansions,
                           lazy_expansion=True)

    elif search_method == "hill_climbing":
        # Terminate when no improvement is found.
        path, _, _ = run_hill_climbing(initial_states=initial_states,
                                       check_goal=lambda _: False,
                                       get_successors=get_successors,
                                       heuristic=heuristic,
                                       early_termination_heuristic_thresh=0,
                                       enforced_depth=hc_enforced_depth)

    else:
        raise NotImplementedError("Unrecognized search_method "
                                  f"{search_method}.")

    # Extract the best seen policy.
    best_ldl = path[-1]

    # To promote generalization, if the search terminated early due to a
    # perfect heuristic, then run a quick hill-climbing search with only
    # delete operators to try to prune the policy a little bit.
    if heuristic(best_ldl) == 0:
        print("Trying to improve the policy by pruning.")
        search_operators = _create_search_operators(predicates,
                                                    operators,
                                                    _generate_plan_examples,
                                                    allow_new_vars,
                                                    delete_only=True)
        heuristic = _create_heuristic(heuristic_name,
                                      predicates,
                                      operators,
                                      train_tasks,
                                      horizon,
                                      demos,
                                      task_planning_heuristic,
                                      max_policy_guided_rollout,
                                      regularize=True)
        initial_states = [best_ldl]
        get_successors = _operators_to_successor_fn(search_operators,
                                                    max_rule_params)
        path, _, _ = run_hill_climbing(initial_states=initial_states,
                                       check_goal=lambda _: False,
                                       get_successors=get_successors,
                                       heuristic=heuristic,
                                       enforced_depth=hc_enforced_depth)
        best_ldl = path[-1]

    return best_ldl


def _create_search_operators(
        predicates: Set[Predicate],
        operators: Set[STRIPSOperator],
        generate_plan_examples: Callable[[LiftedDecisionList],
                                         Iterator[Trajectory]],
        allow_new_vars: bool = True,
        delete_only: bool = False) -> List[_PG3SearchOperator]:
    if delete_only:
        search_operator_classes = [
            _DeleteRulePG3SearchOperator,
            _DeleteConditionPG3SearchOperator,
        ]
    else:
        search_operator_classes = [
            _BottomUpPG3SearchOperator,
            # TODO uncomment
            # _AddRulePG3SearchOperator,
            # _AddConditionPG3SearchOperator,
            # _DeleteRulePG3SearchOperator,
            # _DeleteConditionPG3SearchOperator,
        ]
    return [
        cls(predicates, operators, generate_plan_examples, allow_new_vars)
        for cls in search_operator_classes
    ]


def _operators_to_successor_fn(search_operators: Sequence[_PG3SearchOperator],
                               max_rule_params: int) -> Callable:

    def get_successors(
        ldl: LiftedDecisionList
    ) -> Iterator[Tuple[_PG3SearchOperator, LiftedDecisionList, float]]:
        for op in search_operators:
            for i, child in enumerate(op.get_successors(ldl)):
                if any(
                        len(rule.parameters) > max_rule_params
                        for rule in child.rules):
                    continue
                yield (op, i), child, 1.0  # cost always 1

    return get_successors


def _create_heuristic(heuristic_name: str,
                      predicates: Set[Predicate],
                      operators: Set[STRIPSOperator],
                      train_tasks: Sequence[Task],
                      horizon: int,
                      demos: Optional[List[List[str]]],
                      task_planning_heuristic: str,
                      max_policy_guided_rollout: int,
                      regularize: bool = False) -> _PG3Heuristic:
    heuristic_name_to_cls: Dict[str, TypingType[_PG3Heuristic]] = {
        "policy_guided": _PolicyGuidedPG3Heuristic,
        "policy_evaluation": _PolicyEvaluationPG3Heuristic,
        "demo_plan_comparison": _DemoPlanComparisonPG3Heuristic,
    }
    cls = heuristic_name_to_cls[heuristic_name]
    return cls(predicates,
               operators,
               train_tasks,
               horizon,
               demos,
               task_planning_heuristic,
               max_policy_guided_rollout,
               regularize=regularize)
